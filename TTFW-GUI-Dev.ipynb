{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d9a632-9a99-42ee-b27f-8878dc143b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import re\n",
    "import json\n",
    "import ipaddress\n",
    "import logging\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "try:\n",
    "    open('_debug.log', 'w').close()\n",
    "    logging.basicConfig(format='%(asctime)s %(message)s', filename='_debug.log', encoding='utf-8', level=logging.DEBUG)\n",
    "    logging.info(\"PROGRAM_INITIALIZED\" + \"#\"*100)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2f4cf1-cc6c-4b55-8e79-7ab28fb23b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rule:\n",
    "    def determine_fields(self):\n",
    "        try:\n",
    "            regex_dict = {\n",
    "                'action'                     : r'^(add|remove|keep)',\n",
    "                'environment'                : r'^(testing|test|production|prod|development|dev|staging)',\n",
    "                'address_singular'           : r'^(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})',\n",
    "                'address_range_shorthand'    : r'^(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}-\\d{1,3})',\n",
    "                'address_range_expanded'     : r'^(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}-\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})',\n",
    "                'address_any'                : r'^(any)',\n",
    "                'address_cidr'               : r'^(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}(\\s)?/(\\s)?\\d{1,2})',\n",
    "                'port_singular'              : r'^(\\d{1,5}\\s)|any|ANY|Any',\n",
    "                'port_range'                 : r'^(\\d{1,5}-\\d{1,5})',\n",
    "                'port_comma_separated'       : r'^(((\\d{1,5})?,( )?\\d{1,5}))+',\n",
    "                'protocol'                   : r'^(tcp|udp)'\n",
    "            }\n",
    "            action = self.regex_match_and_trim(regex_dict['action'])\n",
    "            from_environment = self.regex_match_and_trim(regex_dict['environment'])\n",
    "            from_address = [match for match in [self.regex_match_and_trim(regex_dict[\"address_cidr\"]), self.regex_match_and_trim(regex_dict[\"address_any\"]), self.regex_match_and_trim(regex_dict[\"address_range_expanded\"]), self.regex_match_and_trim(regex_dict[\"address_range_shorthand\"]), self.regex_match_and_trim(regex_dict['address_singular'])] if match is not None][0]  # Checks for match of each type -- order of checking is important and you may still get a match of singular for expanded etc.\n",
    "            from_port = [match for match in [self.regex_match_and_trim(regex_dict[\"port_comma_separated\"]), self.regex_match_and_trim(regex_dict[\"port_range\"]), self.regex_match_and_trim(regex_dict[\"port_singular\"])] if match is not None][0]  # Checks for match of each type -- order of checking is important and you may still get a match of singular for range etc.\n",
    "            to_environment = self.regex_match_and_trim(regex_dict['environment'])\n",
    "            to_address = [match for match in [self.regex_match_and_trim(regex_dict[\"address_cidr\"]), self.regex_match_and_trim(regex_dict[\"address_any\"]), self.regex_match_and_trim(regex_dict[\"address_range_expanded\"]), self.regex_match_and_trim(regex_dict[\"address_range_shorthand\"]), self.regex_match_and_trim(regex_dict['address_singular'])] if match is not None][0]  # Checks for match of each type -- order of checking is important and you may still get a match of singular for expanded etc.\n",
    "            to_port = [match.strip() for match in [self.regex_match_and_trim(regex_dict[\"port_comma_separated\"]), self.regex_match_and_trim(regex_dict[\"port_range\"]), self.regex_match_and_trim(regex_dict[\"port_singular\"])] if match is not None][0]  # Checks for match of each type -- order of checking is important and you may still get a match of singular for range etc.\n",
    "            protocol = self.regex_match_and_trim(regex_dict['protocol'])\n",
    "            try:\n",
    "                for field in [action, from_environment, from_address, from_port, to_environment, to_address, to_port, protocol]:\n",
    "                    assert field is not None\n",
    "            except AssertionError:\n",
    "                logging.error(f\"ERROR:Rule:determine_fields:$        failed to determine field\")\n",
    "                pass_fail_indicator.value = False\n",
    "                \n",
    "            logging.info(f\"INFO:Rule:determine_fields:$         determined : '{action}','{from_environment}','{from_address}','{from_port}','{to_environment}','{to_address}','{to_port}','{protocol}'\")\n",
    "        except Exception as error:\n",
    "            logging.error(f\"ERROR:Rule:determine_fields:$        exception {error}\")\n",
    "            pass_fail_indicator.value = False\n",
    "        return action, from_environment, from_address, from_port, to_environment, to_address, to_port, protocol\n",
    "\n",
    "    @staticmethod\n",
    "    def regex_match_and_trim(expression):\n",
    "        try:\n",
    "            global working_rule\n",
    "            match = re.compile(expression, re.IGNORECASE).match(working_rule)\n",
    "            if match:\n",
    "                working_rule = (working_rule[match.span()[1]::]).strip()\n",
    "                return match[0]\n",
    "            elif not match:\n",
    "                return None\n",
    "        except Exception as error:\n",
    "            logging.error(f\"ERROR:Rule:regex_match_and_trim:$ exception {error}\", exc_info=True)\n",
    "            pass_fail_indicator.value = False\n",
    "\n",
    "    def __init__(self, rule):\n",
    "        global working_rule\n",
    "        working_rule = rule\n",
    "        self.rule = rule\n",
    "        self.determined = self.determine_fields()\n",
    "        self.action = self.determined[0]\n",
    "        self.from_environment = self.determined[1]\n",
    "        self.from_address = self.determined[2]\n",
    "        self.from_port = self.determined[3]\n",
    "        self.to_environment = self.determined[4]\n",
    "        self.to_address = self.determined[5]\n",
    "        self.to_port = self.determined[6]\n",
    "        self.protocol = self.determined[7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f9f935-3eff-4a11-b582-905d81b3d986",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Correlation:\n",
    "    @staticmethod\n",
    "    def isolate_address(address):\n",
    "        try:\n",
    "            if 'any' in address.lower():\n",
    "                return None\n",
    "            elif 'any' not in address.lower():\n",
    "                address_expression = re.compile(r'\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3}')\n",
    "                match = address_expression.search(address)\n",
    "                if match:\n",
    "                    return match[0]\n",
    "        except Exception as error:\n",
    "            logging.error(f\"ERROR:Correlation:isolate_address:$  exception {error}\", exc_info=True)\n",
    "            pass_fail_indicator.value = False\n",
    "\n",
    "    def correlate_address(self):\n",
    "        asa_address, asa_context, asa_access_list = None, None, None\n",
    "        from_address = self.isolate_address(self.from_address)\n",
    "        to_address = self.isolate_address(self.to_address)\n",
    "        with open(self.correlations_file, 'r') as file:\n",
    "            correlations_config = json.load(file)\n",
    "            for asa in correlations_config.keys():\n",
    "                for context in correlations_config[asa].keys():\n",
    "                    for acl in correlations_config[asa][context].keys():\n",
    "                        acl_network = ipaddress.ip_network(correlations_config[asa][context][acl])\n",
    "                        if from_address:\n",
    "                            from_address = ipaddress.IPv4Address(from_address)\n",
    "                            if from_address in acl_network:\n",
    "                                if \"VLAN\" in acl:\n",
    "                                    asa_address = asa\n",
    "                                    asa_context = context\n",
    "                                    asa_access_list = acl\n",
    "                                    logging.info(f\"INFO:Correlation:correlate_address:$ correlated : {from_address} in {acl_network} at {asa_address} > {asa_context:} > {asa_access_list}\")\n",
    "                                    return asa_address, asa_context, asa_access_list\n",
    "            for asa in correlations_config.keys():\n",
    "                for context in correlations_config[asa].keys():\n",
    "                    for acl in correlations_config[asa][context].keys():\n",
    "                        acl_network = ipaddress.ip_network(correlations_config[asa][context][acl])\n",
    "                        if to_address:\n",
    "                            to_address = ipaddress.IPv4Address(to_address)\n",
    "                            if to_address in acl_network:\n",
    "                                if \"SVI\" in acl:\n",
    "                                    asa_address = asa\n",
    "                                    asa_context = context\n",
    "                                    asa_access_list = acl\n",
    "                                    logging.info(f\"INFO:Correlation:correlate_address:$ correlated : {to_address} in {acl_network} at {asa_address} > {asa_context:} > {asa_access_list}\")\n",
    "                                    return asa_address, asa_context, asa_access_list\n",
    "\n",
    "    def __init__(self, from_address, to_address, correlations):\n",
    "        self.correlations_file = correlations\n",
    "        self.from_address = from_address\n",
    "        self.to_address = to_address\n",
    "        self.correlation = self.correlate_address()\n",
    "        self.asa_address = self.correlation[0]\n",
    "        self.asa_context = self.correlation[1]\n",
    "        self.asa_access_list = self.correlation[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980e3d91-0e11-4533-96d7-b965556fc60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grouper:\n",
    "    def split_multiple_ports(self):\n",
    "        # If comma-separated ports are detected in the rule, proceed to duplicate this rule in the dataframe for each port. Then drop the original row containing the comma-separated ports.\n",
    "        try:\n",
    "            for index, rule in self.master_frame.iterrows():\n",
    "                if ',' in rule['To_Port']:\n",
    "                    logging.info(f\"INFO:Grouper:split_multiple_ports:$  comma-sep ports detected : {rule['Action']} {rule['From_Environment']} {rule['From_Address']} {rule['From_Port']} {rule['To_Environment']} {rule['To_Address']} {rule['To_Port']} {rule['Protocol']}\")\n",
    "                    rule_copy = rule\n",
    "                    logging.info(f\"INFO:Grouper:split_multiple_ports:$  dropping rule at index {index}  --  {rule['Action']} {rule['From_Environment']} {rule['From_Address']} {rule['From_Port']} {rule['To_Environment']} {rule['To_Address']} {rule['To_Port']} {rule['Protocol']}\")\n",
    "                    self.master_frame.drop(index, inplace=True)\n",
    "                    logging.info(f\"INFO:Grouper:split_multiple_ports:$  comma-sep rule dropped; copy retained\")\n",
    "                    all_ports = rule_copy['To_Port'].split(',')\n",
    "                    all_ports = [port.strip(' ') for port in all_ports]\n",
    "                    for port in all_ports:\n",
    "                        rule_copy['To_Port'] = port\n",
    "                        self.master_frame = self.master_frame.append(rule_copy)\n",
    "                        logging.info(f\"INFO:Grouper:split_multiple_ports:$  added rule copy with port : {port}\")\n",
    "            self.master_frame.reset_index(drop=True, inplace=True)\n",
    "        except Exception as error:\n",
    "            logging.error(f\"ERROR:Grouper:split_multiple_ports:$ exception {error}\", exc_info=True)\n",
    "            pass_fail_indicator.value = False\n",
    "\n",
    "    def remove_duplicates(self):\n",
    "        # Removes duplicate rows / rules if present. Ignores From_Environment and To_Environment.\n",
    "        try:\n",
    "            temp_frame = self.master_frame[self.master_frame.duplicated(subset=['Action', 'From_Address', 'From_Port', 'To_Address', 'To_Port', 'Protocol'], keep='last')]\n",
    "            self.master_frame = self.master_frame.drop(temp_frame.index)\n",
    "            self.master_frame = self.master_frame.reset_index(drop=True)\n",
    "            if len(temp_frame) > 0:\n",
    "                logging.info(f\"INFO:Grouper:remove_duplicates:$     removed duplicates : {len(temp_frame)}\")\n",
    "                logging.info(f\"---------------------------------------------REMOVED DUPLICATES-----------------------------------------\\n{temp_frame}\")\n",
    "        except Exception as error:\n",
    "            logging.error(f\"ERROR:Grouper:remove_duplicates:$    exception {error}\", exc_info=True)\n",
    "            pass_fail_indicator.value = False\n",
    "\n",
    "    def group_to_address(self):\n",
    "        # Groups rows / rules that share important fields (i.e. not From/To_Environment or From_Port) logically. From_Port is always assumed ephemeral and is therefore ignored.\n",
    "        # Produces Cisco ASA object-group of TO addresses. Example: 'access-list acl-25-VLAN line 1 extended permit TCP host 25.25.25.3 object-group network_object_1 eq 8443'\n",
    "        try:\n",
    "            temp_frame = self.master_frame[self.master_frame.duplicated(subset=['Action', 'From_Address', 'Protocol', 'ASA_ACL', 'To_Port'], keep=False)]\n",
    "            if temp_frame.empty:\n",
    "                return None\n",
    "            acl_list = []\n",
    "            for acl_field in temp_frame['ASA_ACL']:\n",
    "                acl_list.append(acl_field)\n",
    "            acl_list = list(set(acl_list))\n",
    "            logging.info(f\"INFO:Grouper:group_to_address:$      object-groupings of To_Address discovered\")\n",
    "            \n",
    "            for acl in acl_list:\n",
    "                object_group_df = (temp_frame.loc[(temp_frame['ASA_ACL'] == acl)]) # If ACL in list matches ACL in rule/row, that's your object-group. Modify this to also verify from_address field is the same since TO address is being grouped\n",
    "                logging.info(f\"---------------------------------------------TO ADDRESS GROUPING / BY ACL---------------------------------\\n{object_group_df}\")\n",
    "                unique_from_address_list = []\n",
    "                for from_address_field in object_group_df['From_Address']:\n",
    "                    unique_from_address_list.append(from_address_field)\n",
    "                unique_from_address_list = list(set(unique_from_address_list))\n",
    "                for unique_from_address in unique_from_address_list:\n",
    "                    singular_object_group_df = (object_group_df.loc[(object_group_df['From_Address'] == unique_from_address)])\n",
    "                    self.groups['To_Address'].append(singular_object_group_df)\n",
    "                    logging.info(f\"---------------------------------------------TO ADDRESS GROUPING / BY GROUPING---------------------------------\\n{singular_object_group_df}\")\n",
    "                self.master_frame.drop((temp_frame.loc[(temp_frame['ASA_ACL'] == acl)]).index, inplace=True) # Drop rows from master frame that are in the temp frame with the current iterated acl in acl_list\n",
    "        except Exception as error:\n",
    "            logging.error(f\"ERROR:Grouper:group_to_address:$     exception {error}\", exc_info=True)\n",
    "            pass_fail_indicator.value = False\n",
    "\n",
    "    def group_from_address(self):\n",
    "        # Groups rows / rules that share important fields (i.e. not From/To_Environment or From_Port) logically. From_Port is always assumed ephemeral and is therefore ignored.\n",
    "        # Produces Cisco ASA object-group of FROM addresses. Example: 'access-list acl-25-VLAN line 1 extended permit TCP object-group network_object_1 host 25.25.25.3 eq 8443'\n",
    "        try:\n",
    "            temp_frame = self.master_frame[self.master_frame.duplicated(subset=['Action', 'To_Address', 'Protocol', 'ASA_ACL', 'To_Port'], keep=False)]\n",
    "            if temp_frame.empty:\n",
    "                return None\n",
    "            acl_list = []\n",
    "            for acl_field in temp_frame['ASA_ACL']:\n",
    "                acl_list.append(acl_field)\n",
    "            acl_list = list(set(acl_list))\n",
    "            logging.info(f\"INFO:Grouper:group_from_address:$      object-groupings of From_Address discovered\")\n",
    "            \n",
    "            for acl in acl_list:\n",
    "                object_group_df = (temp_frame.loc[(temp_frame['ASA_ACL'] == acl)]) # If ACL in list matches ACL in rule/row, that's your object-group. Modify this to also verify to_address field is the same since TO address is being grouped\n",
    "                logging.info(f\"---------------------------------------------FROM ADDRESS GROUPING / BY ACL---------------------------------\\n{object_group_df}\")\n",
    "                unique_to_address_list = []\n",
    "                for to_address_field in object_group_df['To_Address']:\n",
    "                    unique_to_address_list.append(to_address_field)\n",
    "                unique_to_address_list = list(set(unique_to_address_list))\n",
    "                for unique_to_address in unique_to_address_list:\n",
    "                    singular_object_group_df = (object_group_df.loc[(object_group_df['To_Address'] == unique_to_address)])\n",
    "                    self.groups['From_Address'].append(singular_object_group_df)\n",
    "                    logging.info(f\"---------------------------------------------FROM ADDRESS GROUPING / BY GROUPING---------------------------------\\n{singular_object_group_df}\")\n",
    "                self.master_frame.drop((temp_frame.loc[(temp_frame['ASA_ACL'] == acl)]).index, inplace=True) # Drop rows to master frame that are in the temp frame with the current iterated acl in acl_list\n",
    "        except Exception as error:\n",
    "            logging.error(f\"ERROR:Grouper:group_from_address:$     exception {error}\", exc_info=True)\n",
    "            pass_fail_indicator.value = False\n",
    "\n",
    "    def group_to_port(self):\n",
    "        try:\n",
    "            temp_frame = self.master_frame[self.master_frame.duplicated(subset=['Action', 'From_Address', 'To_Address', 'Protocol', 'ASA_ACL'], keep=False)]\n",
    "            if temp_frame.empty:\n",
    "                return None\n",
    "            acl_list = []\n",
    "            for acl_field in temp_frame['ASA_ACL']:\n",
    "                acl_list.append(acl_field)\n",
    "            acl_list = list(set(acl_list))\n",
    "            logging.info(f\"INFO:Grouper:group_from_address:$    object-groupings of To_Port discovered\")\n",
    "            for acl in acl_list:\n",
    "                object_group_df = (temp_frame.loc[(temp_frame['ASA_ACL'] == acl)])\n",
    "                logging.info(f\"---------------------------------------------TO PORT GROUPING---------------------------------------\\n{object_group_df}\")\n",
    "                self.master_frame.drop((temp_frame.loc[(temp_frame['ASA_ACL'] == acl)]).index, inplace=True)\n",
    "                self.groups['To_Port'].append(object_group_df)\n",
    "        except Exception as error:\n",
    "            logging.error(f\"ERROR:Grouper:group_to_port:$        exception {error}\", exc_info=True)\n",
    "            pass_fail_indicator.value = False\n",
    "\n",
    "    def group_remainder(self):\n",
    "        try:\n",
    "            if self.master_frame.empty:\n",
    "                return None\n",
    "            logging.info(f\"INFO:Grouper:group_remainder:$      remaining ungrouped rules discovered\")\n",
    "            logging.info(f\"------------------------------------------REMAINING UNGROUPED RULES------------------------------------\\n{self.master_frame}\")\n",
    "            self.groups['Host_Singular'].append(self.master_frame)\n",
    "        except Exception as error:\n",
    "            logging.error(f\"ERROR:Grouper:group_remainder:$      exception {error}\", exc_info=True)\n",
    "            pass_fail_indicator.value = False\n",
    "\n",
    "    def __init__(self, dataframe):\n",
    "        self.groups = {\n",
    "            'From_Address': [],\n",
    "            'To_Address': [],\n",
    "            'To_Port': [],\n",
    "            'Host_Singular': []\n",
    "        }\n",
    "        self.master_frame = dataframe\n",
    "        self.split_multiple_ports()\n",
    "        self.remove_duplicates()\n",
    "        self.group_from_address()\n",
    "        self.group_to_address()\n",
    "        self.group_to_port()\n",
    "        self.group_remainder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40787d2-736d-443f-95bf-763f903c6596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Commands:\n",
    "    def get_settings(self, key):\n",
    "        try:\n",
    "            with open(self.settings_file, 'r') as file:\n",
    "                settings = json.load(file)\n",
    "                return settings[key]\n",
    "        except Exception as error:\n",
    "            logging.error(f\"ERROR:Commands:get_settings:$        exception {error}\", exc_info=True)\n",
    "            pass_fail_indicator.value = False\n",
    "\n",
    "    def increment_settings(self, key):\n",
    "        try:\n",
    "            with open(self.settings_file, \"r\") as file:\n",
    "                data = json.load(file)\n",
    "                file.close()\n",
    "            data[key] += 1\n",
    "            with open(\"settings.json\", \"w\") as file:\n",
    "                write_to_json = json.dump(data, file)\n",
    "                logging.info(f\"INFO:Commands:increment_settings:$ incremented settings : {key} in {self.settings_file}\")\n",
    "                file.close()\n",
    "        except Exception as error:\n",
    "            logging.error(f\"ERROR:Commands:increment_settings:$  exception {error}\", exc_info=True)\n",
    "            pass_fail_indicator.value = False\n",
    "\n",
    "    @staticmethod\n",
    "    def cidr_to_netmask(cidr):\n",
    "        try:\n",
    "            cidr = int(cidr)\n",
    "            mask = (0xffffffff >> (32 - cidr)) << (32 - cidr)\n",
    "            return (str((0xff000000 & mask) >> 24) + '.' +\n",
    "                    str((0x00ff0000 & mask) >> 16) + '.' +\n",
    "                    str((0x0000ff00 & mask) >> 8) + '.' +\n",
    "                    str((0x000000ff & mask)))\n",
    "        except Exception as error:\n",
    "            logging.error(f\"ERROR:Commands:cidr_to_netmask:$     exception {error}\", exc_info=True)\n",
    "            pass_fail_indicator.value = False\n",
    "\n",
    "\n",
    "    def generate_from_address(self, object_group_df):\n",
    "        try:\n",
    "            command_count = len(self.commands)\n",
    "            self.increment_settings('object_network_index')\n",
    "            object_network_prefix = self.get_settings('object_network_prefix')\n",
    "            object_network_index = self.get_settings('object_network_index')\n",
    "            for dataframe in self.groups['From_Address']:\n",
    "                for rule in dataframe.index:\n",
    "                    rule = dataframe.loc[rule]\n",
    "                    break\n",
    "            self.commands.append(f\"changeto context {object_group_df.iloc[0]['ASA_Context']}\")\n",
    "            self.commands.append(f\"conf t\")\n",
    "            self.commands.append(f\"object-group network {object_network_prefix}{object_network_index}\")\n",
    "            for row in object_group_df.index:\n",
    "                rule = object_group_df.loc[row]\n",
    "                address = rule['From_Address']\n",
    "                self.address_logic_object_group(address)\n",
    "            self.commands.append(\"exit\")\n",
    "            self.commands.append(f\"access-list {rule['ASA_ACL']} line 1 extended permit {rule['Protocol']} object-group {object_network_prefix + str(object_network_index)} host {rule['To_Address']} eq {rule['To_Port']}\")\n",
    "            if self.remark:\n",
    "                self.commands.append(f\"access-list {rule['ASA_ACL']} line 1 remark {self.remark}\")\n",
    "            self.commands.append(\"\\n\")\n",
    "            command_count = len(self.commands) - command_count\n",
    "            logging.info(f\"INFO:Commands:generate_from_address:$ generated {command_count} From_Address grouped commands : \\n{'.'*30}{(os.linesep + '.'*30).join([command for command in self.commands[-(command_count):]])}\")\n",
    "        except Exception as error:\n",
    "            logging.error(f\"ERROR:Commands:generate_from_address:$ exception {error}\", exc_info=True)\n",
    "            pass_fail_indicator.value = False\n",
    "\n",
    "    def generate_to_address(self, object_group_df):\n",
    "        try:\n",
    "            command_count = len(self.commands)\n",
    "            self.increment_settings('object_network_index')\n",
    "            object_network_prefix = self.get_settings('object_network_prefix')\n",
    "            object_network_index = self.get_settings('object_network_index')\n",
    "            for dataframe in self.groups['To_Address']:\n",
    "                for rule in dataframe.index:\n",
    "                    rule = dataframe.loc[rule]\n",
    "                    break\n",
    "            self.commands.append(f\"changeto context {object_group_df.iloc[0]['ASA_Context']}\")\n",
    "            self.commands.append(f\"conf t\")\n",
    "            self.commands.append(f\"object-group network {object_network_prefix}{object_network_index}\")\n",
    "            for row in object_group_df.index:\n",
    "                rule = object_group_df.loc[row]\n",
    "                address = rule['To_Address']\n",
    "                self.address_logic_object_group(address)\n",
    "            self.commands.append(\"exit\")\n",
    "            self.commands.append(f\"access-list {rule['ASA_ACL']} line 1 extended permit {rule['Protocol']} host {rule['From_Address']} object-group {object_network_prefix + str(object_network_index)} eq {rule['To_Port']}\")\n",
    "            if self.remark:\n",
    "                self.commands.append(f\"access-list {rule['ASA_ACL']} line 1 remark {self.remark}\")\n",
    "            self.commands.append(\"\\n\")\n",
    "            command_count = len(self.commands) - command_count\n",
    "            logging.info(f\"INFO:Commands:generate_to_address:$ generated {command_count} To_Address grouped commands : \\n{'.'*30}{(os.linesep + '.'*30).join([command for command in self.commands[-(command_count):]])}\")\n",
    "        except Exception as error:\n",
    "            logging.error(f\"ERROR:Commands:generate_to_address:$ exception {error}\", exc_info=True)\n",
    "            pass_fail_indicator.value = False\n",
    "\n",
    "    def generate_to_port(self, object_group_df):\n",
    "        try:\n",
    "            command_count = len(self.commands)\n",
    "            port_list = []\n",
    "            self.increment_settings('object_service_index')\n",
    "            object_service_prefix = self.get_settings('object_service_prefix')\n",
    "            object_service_index = self.get_settings('object_service_index')\n",
    "            self.commands.append(f\"changeto context {object_group_df.iloc[0]['ASA_Context']}\")\n",
    "            self.commands.append('conf t')\n",
    "            self.commands.append(f'object-group service {object_service_prefix}{object_service_index}')\n",
    "            # Below appends port_list with all ports, then removes duplicates to avoid ASA duplicate object error\n",
    "            for row in object_group_df.index:\n",
    "                rule = object_group_df.loc[row]\n",
    "                port = rule['To_Port']\n",
    "                port_list.append(port)\n",
    "            port_list = list(set(port_list))\n",
    "            for i in port_list:\n",
    "                self.commands.append(f\"service-object {rule['Protocol']} destination eq {i}\")\n",
    "            self.commands.append('exit')\n",
    "            a = object_group_df[object_group_df.duplicated(subset=['Action', 'From_Address', 'From_Port', 'To_Address', 'Protocol'], keep='last')]\n",
    "            obj_group_df = object_group_df.drop(a.index)\n",
    "            obj_group_df = obj_group_df.reset_index(drop=True)\n",
    "            for row in obj_group_df.index:\n",
    "                df_rule = obj_group_df.loc[row]\n",
    "                self.commands.append(f\"access-list {df_rule['ASA_ACL']} line 1 extended permit object-group {object_service_prefix + str(object_service_index)} host {df_rule['From_Address']} host {df_rule['To_Address']}\")\n",
    "            if self.remark:\n",
    "                self.commands.append(f\"access-list {df_rule['ASA_ACL']} line 1 remark {self.remark}\")\n",
    "            self.commands.append(\"\\n\")\n",
    "            command_count = len(self.commands) - command_count\n",
    "            logging.info(f\"INFO:Commands:generate_to_port:$ generated {command_count} To_Port grouped commands : \\n{'.'*30}{(os.linesep + '.'*30).join([command for command in self.commands[-(command_count):]])}\")\n",
    "        except Exception as error:\n",
    "            logging.error(f\"ERROR:Commands:generate_to_port:$    exception {error}\", exc_info=True)\n",
    "            pass_fail_indicator.value = False\n",
    "\n",
    "    def generate_singular(self, object_group_df):\n",
    "        try:\n",
    "            command_count = len(self.commands)\n",
    "            acl_list = []\n",
    "            asa_address = ''\n",
    "            asa_context = ''\n",
    "            temp_group_dataframe = pd.DataFrame(columns=['Command', 'ASA_ACL', 'ASA_Context', 'ASA_Address'])\n",
    "            for row in object_group_df.index:\n",
    "                df_rule = object_group_df.loc[row]\n",
    "                acl_list.append(df_rule['ASA_ACL'])\n",
    "            acl_list = list(set(acl_list))\n",
    "            for acl in acl_list:\n",
    "                logging.info(f\"INFO:Commands:generate_singular:$ singular command generation working within ACL: {acl}\")\n",
    "                for row in object_group_df.index:\n",
    "                    logging.info(f\"INFO:Commands:generate_singular:$ singular command generation iterating rule : {row}\")\n",
    "                    df_rule = object_group_df.loc[row]\n",
    "                    if df_rule['ASA_ACL'] == acl:\n",
    "                        data = {\"Command\": f\"access-list {df_rule['ASA_ACL']} line 1 extended permit {df_rule['Protocol']} {self.address_logic_singular(df_rule['From_Address'])} {self.address_logic_singular(df_rule['To_Address'])} eq {df_rule['To_Port']}\", \"ASA_ACL\": df_rule['ASA_ACL'], \"ASA_Context\": df_rule['ASA_Context'], \"ASA_Address\": df_rule['ASA_Address']}\n",
    "                        logging.info(f\"INFO:Commands:generate_singular:$ singular command generated commands : {data['Command']}\")\n",
    "                        temp_group_dataframe = temp_group_dataframe.append(data, ignore_index=True)\n",
    "            temp_group_dataframe.groupby(\"ASA_Address\")\n",
    "            logging.info(f\"INFO:Commands:generate_singular:$ groupy complete -- begin row iteration and command append\")\n",
    "            logging.info(f\"-\"*80)\n",
    "            logging.info(f\"\\n\\n{temp_group_dataframe}\\n\\n\")\n",
    "            for row in temp_group_dataframe.index:\n",
    "                df_rule = temp_group_dataframe.loc[row]\n",
    "                logging.info(f\"\\n\\n{df_rule}\\n\\n\")\n",
    "                if df_rule['ASA_Address'] == asa_address:\n",
    "                    pass\n",
    "                elif df_rule['ASA_Address'] != asa_address:\n",
    "                    asa_address = df_rule['ASA_Address']\n",
    "                    self.commands.append(f\"ssh {asa_address}\")\n",
    "                if df_rule['ASA_Context'] == asa_context:\n",
    "                    pass\n",
    "                elif df_rule['ASA_Context'] != asa_context:\n",
    "                    asa_context = df_rule['ASA_Context']\n",
    "                    self.commands.append(f\"changeto context {df_rule['ASA_Context']}\")\n",
    "                    self.commands.append(\"conf t\")\n",
    "                self.commands.append(df_rule['Command'])\n",
    "                logging.info(f\"INFO:Commands:generate_singular:$ command append to commands : {df_rule['Command']}\")\n",
    "            if self.remark:\n",
    "                self.commands.append(f\"access-list {df_rule['ASA_ACL']} line 1 remark {self.remark}\")\n",
    "            self.commands.append(\"\\n\")\n",
    "            command_count = len(self.commands) - command_count\n",
    "            logging.info(f\"INFO:Commands:generate_singular:$ generated {command_count} singular host commands : \\n{'.'*30}{(os.linesep + '.'*30).join([command for command in self.commands[-(command_count):]])}\")\n",
    "        except Exception as error:\n",
    "            logging.error(f\"ERROR:Commands:generate_singular:$   exception {error}\", exc_info=True)\n",
    "            pass_fail_indicator.value = False\n",
    "\n",
    "    def command_handler(self):\n",
    "        try:\n",
    "            if self.groups['From_Address']:\n",
    "                for dataframe in self.groups['From_Address']:\n",
    "                    self.generate_from_address(dataframe)\n",
    "            if self.groups['To_Address']:\n",
    "                for dataframe in self.groups['To_Address']:\n",
    "                    self.generate_to_address(dataframe)\n",
    "            if self.groups['To_Port']:\n",
    "                for dataframe in self.groups['To_Port']:\n",
    "                    self.generate_to_port(dataframe)\n",
    "            if self.groups['Host_Singular']:\n",
    "                for dataframe in self.groups['Host_Singular']:\n",
    "                    self.generate_singular(dataframe)\n",
    "        except Exception as error:\n",
    "            logging.error(f\"ERROR:Commands:command_handler:$     exception {error}\", exc_info=True)\n",
    "            pass_fail_indicator.value = False\n",
    "\n",
    "    def address_logic_object_group(self, address):\n",
    "        try:\n",
    "            if '-' not in address:  # Singular\n",
    "                self.commands.append(f\"network-object host {address}\")\n",
    "            if '-' in address and address.count('.') > 3:  # Range-expanded\n",
    "                self.commands.append(f\"network-object range {address}\")\n",
    "            if '-' in address and address.count('.') == 3:  # Range-shorthand\n",
    "                self.commands.append(f\"network-object range {address}\")\n",
    "            if '/' in address:  # Subnet\n",
    "                address = address.split(\"/\")\n",
    "                address[1] = self.cidr_to_netmask(address[1])\n",
    "                self.commands.append(f\"network-object subnet {address[0]} {address[1]}\")\n",
    "        except Exception as error:\n",
    "            logging.error(f\"ERROR:Commands:address_logic_object_group:$ exception {error}\", exc_info=True)\n",
    "            pass_fail_indicator.value = False\n",
    "\n",
    "    def address_logic_singular(self, address):\n",
    "        try:\n",
    "            if '/' in address:  # Subnet\n",
    "                address = address.split(\"/\")\n",
    "                address[1] = self.cidr_to_netmask(address[1])\n",
    "                return f\"{address[0]} {address[1]}\"\n",
    "            if '-' not in address and address.count('.') == 3:  # Singular\n",
    "                return f\"host {address}\"\n",
    "            if '-' in address and address.count('.') > 3:  # Range-expanded\n",
    "                return address\n",
    "            if '-' in address and address.count('.') == 3:  # Range-shorthand\n",
    "                return address\n",
    "        except Exception as error:\n",
    "            logging.error(f\"ERROR:Commands:address_logic_singular:$ exception {error}\", exc_info=True)\n",
    "            pass_fail_indicator.value = False\n",
    "\n",
    "    def __init__(self, groups, remark, settings_file):\n",
    "        self.settings_file = settings_file\n",
    "        self.remark = remark\n",
    "        self.groups = groups\n",
    "        self.commands = []\n",
    "        self.command_handler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a4d749-f6ec-43ec-852c-f9b347f664ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def port_names_to_port_numbers(raw_rules):\n",
    "    try:\n",
    "        port_names = {'echo': '7', 'discard': '9', 'systat': '11', 'daytime': '13', 'qotd': '17', 'chargen': '19', 'ftp-data': '20', 'ftp': '21', 'ssh': '22', 'telnet': '23', 'smtp': '25', 'time': '37', 'rlp': '39', 'nameserver': '42', 'nicname': '43', 'domain': '53', 'bootps': '67', 'bootpc': '68', 'tftp': '69', 'gopher': '70', 'finger': '79', 'http': '80', 'hosts2-ns': '81', 'kerberos': '88',\n",
    "                        'hostname': '101', 'iso-tsap': '102', 'rtelnet': '107', 'pop2': '109', 'pop3': '110', 'sunrpc': '111', 'auth': '113', 'uucp-path': '117', 'sqlserv': '118', 'nntp': '119', 'ntp': '123', 'epmap': '135', 'netbios-ns': '137', 'netbios-dgm': '138', 'netbios-ssn': '139', 'imap': '143', 'sql-net': '150', 'sqlsrv': '156', 'pcmail-srv': '158', 'snmp': '161', 'snmptrap': '162',\n",
    "                        'print-srv': '170', 'bgp': '179', 'irc': '194', 'ipx': '213', 'rtsps': '322', 'mftp': '349', 'ldap': '389', 'https': '443', 'microsoft-ds': '445', 'kpasswd': '464', 'isakmp': '500', 'crs': '507', 'exec': '512', 'login': '513', 'cmd': '514', 'printer': '515', 'talk': '517', 'ntalk': '518', 'efs': '520', 'ulp': '522', 'timed': '525', 'tempo': '526', 'irc-serv': '529',\n",
    "                        'courier': '530', 'conference': '531', 'netnews': '532', 'netwall': '533', 'uucp': '540', 'klogin': '543', 'kshell': '544', 'dhcpv6-client': '546', 'dhcpv6-server': '547', 'afpovertcp': '548', 'new-rwho': '550', 'rtsp': '554', 'remotefs': '556', 'rmonitor': '560', 'monitor': '561', 'nntps': '563', 'whoami': '565', 'ms-shuttle': '568', 'ms-rome': '569', 'http-rpc-epmap': '593',\n",
    "                        'hmmp-ind': '612', 'hmmp-op': '613', 'ldaps': '636', 'doom': '666', 'msexch-routing': '691', 'kerberos-adm': '749', 'kerberos-iv': '750', 'mdbs_daemon': '800', 'ftps-data': '989', 'ftps': '990', 'telnets': '992', 'imaps': '993', 'ircs': '994', 'pop3s': '995', 'activesync': '1034', 'kpop': '1109', 'nfsd-status': '1110', 'nfa': '1155', 'phone': '1167', 'opsmgr': '1270',\n",
    "                        'ms-sql-s': '1433', 'ms-sql-m': '1434', 'ms-sna-server': '1477', 'ms-sna-base': '1478', 'wins': '1512', 'ingreslock': '1524', 'stt': '1607', 'l2tp': '1701', 'pptconference': '1711', 'pptp': '1723', 'msiccp': '1731', 'remote-winsock': '1745', 'ms-streaming': '1755', 'msmq': '1801', 'radius': '1812', 'radacct': '1813', 'msnp': '1863', 'ssdp': '1900', 'close-combat': '1944',\n",
    "                        'nfsd': '2049', 'knetd': '2053', 'mzap': '2106', 'qwave': '2177', 'directplay': '2234', 'ms-olap3': '2382', 'ms-olap4': '2383', 'ms-olap1': '2393', 'ms-olap2': '2394', 'ms-theater': '2460', 'wlbs': '2504', 'ms-v-worlds': '2525', 'sms-rcinfo': '2701', 'sms-xfer': '2702', 'sms-chat': '2703', 'sms-remctrl': '2704', 'msolap-ptp2': '2725', 'icslap': '2869', 'cifs': '3020',\n",
    "                        'xbox': '3074', 'ms-dotnetster': '3126', 'ms-rule-engine': '3132', 'msft-gc': '3268', 'msft-gc-ssl': '3269', 'ms-cluster-net': '3343', 'ms-wbt-server': '3389', 'ms-la': '3535', 'pnrp-port': '3540', 'teredo': '3544', 'p2pgroup': '3587', 'ws-discovery': '3702', 'dvcprov-port': '3776', 'msfw-control': '3847', 'msdts1': '3882', 'sdp-portmapper': '3935', 'net-device': '4350',\n",
    "                        'ipsec-msft': '4500', 'llmnr': '5355', 'wsd': '5357', 'rrac': '5678', 'dccm': '5679', 'ms-licensing': '5720', 'directplay8': '6073', 'ms-do': '7680', 'man': '9535', 'rasadv': '9753', 'imip-channels': '11320'}\n",
    "        clean_input = raw_rules\n",
    "        for name in port_names.keys():\n",
    "            if name in raw_rules.lower():\n",
    "                clean_input = clean_input.lower().replace(name, port_names[name])\n",
    "        return dirtywords_to_portnumbers(clean_input)\n",
    "    except Exception as error:\n",
    "        logging.error(f\"ERROR:__main__:port_names_to_port_numbers:$ exception {error}\", exc_info=True)\n",
    "        pass_fail_indicator.value = False\n",
    "        \n",
    "def dirtywords_to_portnumbers(clean_input):\n",
    "    try:\n",
    "        dirty_words = {\n",
    "            # After initial conversion of service names to numbers, some shared-name services get stepped on--this is a dictionary for replacing those mistakes.\n",
    "            # Comments are in format: actual-service | service-causing-mistake | actual-port-number\n",
    "            't21': '69',  # tftp | ftp | 69\n",
    "            'r23': '107',  # rtelnet | telnet | 107\n",
    "            '161trap': '162',  # snmp-trap | snmp | 162\n",
    "            'm21': '349',  # mftp | ftp | 349\n",
    "            '80s': '443',  # https | http | 443\n",
    "            'n517': '518',  # ntalk | talk | 518\n",
    "            '194-serv': '529',  # irc-serv | irc | 529\n",
    "            'k513': '543',  # klogin | login | 529\n",
    "            'remot520': '556',  # remotefs | efs | 556\n",
    "            '119s': '563',  # nntps | nntp | 563\n",
    "            '80-rpc-135': '593',  # http-rpc-epmap | http & epmap | 593\n",
    "            '389s': '636',  # ldaps | ldap | 636\n",
    "            '88-adm': '749',  # kerberos-adm | kerberos | 749\n",
    "            '88-iv': '750',  # kerberos-iv | kerberos | 750\n",
    "            '21s-data': '989',  # ftps-data | ftp | 989\n",
    "            '21s': '990',  # ftps | ftp | 990\n",
    "            '23s': '992',  # telnets | telnet | 992\n",
    "            '143s': '993',  # imaps | imap | 993\n",
    "            '194s': '994',  # ircs | irc | 994\n",
    "            '110s': '995'  # pop3s | pop3 | 995\n",
    "            }\n",
    "        for key in dirty_words:\n",
    "            if key in clean_input:\n",
    "                clean_input = clean_input.replace(key, dirty_words[key])\n",
    "        raw_input_textarea.value=clean_input\n",
    "        return clean_input\n",
    "    except Exception as error:\n",
    "        logging.error(f\"ERROR:__main__:dirtywords_to_portnumbers:$ exception {error}\", exc_info=True)\n",
    "        pass_fail_indicator.value = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743d01f1-005c-442e-af53-7aa3d6ffc722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder dataframe for display formatting\n",
    "rule_dataframe = pd.DataFrame(columns=['Action', 'From_Environment', 'From_Address', 'From_Port', 'To_Environment', 'To_Address', 'To_Port', 'Protocol', 'ASA_Address', 'ASA_Context', 'ASA_ACL'])\n",
    "\n",
    "def generate_commands():\n",
    "    try:\n",
    "        global rule_dataframe\n",
    "        global generated_commands\n",
    "        rules = raw_input_textarea.value.split('\\n')\n",
    "        df = pd.DataFrame()\n",
    "        line_num = 0\n",
    "        for rule in rules:\n",
    "            line_num += 1\n",
    "            logging.info(\"-\"*15)\n",
    "            logging.info(f\"INFO:__main__:generate_commands:$    begin line {line_num} : {rule}\")\n",
    "            \n",
    "            try:\n",
    "                determined = Rule(rule)\n",
    "            except Exception as error:\n",
    "                logging.error(f\"ERROR:__main__:generate_commands: exception {error} ----> FAILED TO DETERMINE : {rule}\", exc_info=True)\n",
    "                pass_fail_indicator.value = False\n",
    "                \n",
    "            try:\n",
    "                correlated = Correlation(determined.from_address, determined.to_address, correlations_file)\n",
    "            except Exception as error:\n",
    "                logging.error(f\"ERROR:__main__:generate_commands: exception {error} ----> FAILED TO CORRELATE : {rule}\", exc_info=True)\n",
    "                pass_fail_indicator.value = False\n",
    "                \n",
    "            df = df.append({\n",
    "                'Action': determined.action,\n",
    "                'From_Environment': determined.from_environment,\n",
    "                'From_Address': determined.from_address,\n",
    "                'From_Port': determined.from_port,\n",
    "                'To_Environment': determined.to_environment,\n",
    "                'To_Address': determined.to_address,\n",
    "                'To_Port': determined.to_port,\n",
    "                'Protocol': determined.protocol,\n",
    "                'ASA_Address': correlated.asa_address,\n",
    "                'ASA_Context': correlated.asa_context,\n",
    "                'ASA_ACL': correlated.asa_access_list\n",
    "            }, ignore_index=True)\n",
    "        rule_dataframe = df\n",
    "        grid[3:, :1] = output_dataframe(rule_dataframe)\n",
    "        \n",
    "        logging.info(\"-\"*15)\n",
    "        logging.info(f\"INFO:__main__:generate_commands:$    determinations and correlations completed -- begin groupings\")\n",
    "        logging.info(\"-\"*15)\n",
    "        \n",
    "        try:\n",
    "            rule_groupings = Grouper(rule_dataframe).groups\n",
    "        except Exception as error:\n",
    "            logging.error(f\"ERROR:__main__:generate_commands: exception {error} ----> FAILED WHILE GROUPING\", exc_info=True)\n",
    "            pass_fail_indicator.value = False\n",
    "        \n",
    "        logging.info(\"-\"*15)\n",
    "        logging.info(f\"INFO:__main__:generate_commands:$    object groupings completed -- begin generating commands\")\n",
    "        logging.info(\"-\"*15)\n",
    "        \n",
    "        try:\n",
    "            generated_commands = Commands(rule_groupings, remark, settings_file).commands\n",
    "        except Exception as error:\n",
    "            logging.error(f\"ERROR:__main__:generate_commands: exception {error} ----> FAILED TO GENERATE COMMANDS\", exc_info=True)\n",
    "            pass_fail_indicator.value = False\n",
    "        \n",
    "    except Exception as error:\n",
    "        logging.error(f\"ERROR:__main__:generate_commands:$   exception {error} ----> UNCAUGHT ERROR\", exc_info=True)\n",
    "        pass_fail_indicator.value = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fc01d4-269e-4229-a99d-3e1a36424e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1: raw_input_textarea\n",
    "\n",
    "raw_input_textarea = widgets.Textarea(placeholder='''PASTE RAW FIREWALL REQUEST -- EXPAND TO VIEW DETAILS\\n\n",
    "Add Production X.X.X.X any Production X.X.X.X 8080 TCP\n",
    "Add Development X.X.X.X/22 any Development X.X.X.X 53 UDP\n",
    "Add Testing X.X.X.X any Testing X.X.X.X-254 HTTPS TCP\n",
    "Add Staging X.X.X.X any Staging X.X.X.X 67, 68 UDP\n",
    "Add Production X.X.X.X any Production X.X.X.X 23 TCP\n",
    "\n",
    "Standard Format (as directly copied):\n",
    "Action From_Environment From_Address From_Port To_Environment To_Address To_Port Protocol\n",
    "\n",
    "Recognizable Formatting:\n",
    "(All fields separated by any amount of whitespace && ignore case)\n",
    "\n",
    "ACTIONS: Add || Remove || Ignore\n",
    "ENVIRONMENTS: Testing  ||  Test  ||  Production  ||  Prod  ||  Development  ||  Dev  ||  Staging  ||  Stag || Ignore\n",
    "ADDRESSES: 192.168.0.1 (Singular) || 192.168.0.1-150 (Range) || 192.168.0.1-192.168.0.150 (Range-Full) || 192.168.0.1/24 (Subnet)\n",
    "PORTS: Any || Ephemeral || 443 (Singular) || 8800-8804 (Range) || 22, 80, 443 (Multiport)\n",
    "PROTOCOLS: TCP || UDP || TCP/UDP || UDP/TCP\n",
    "''', layout=widgets.Layout(width='auto', height='300px'))\n",
    "#display(raw_input_textarea)\n",
    "\n",
    "raw_input_textarea_interactive = widgets.interactive(port_names_to_port_numbers, raw_rules=raw_input_textarea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5e1186-cb4b-4b79-af17-cc0fd6b76dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2: correlations_text\n",
    "correlations_text = widgets.Text(\n",
    "    placeholder='/filepath/to/correlations.json',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='auto', height='auto')\n",
    ")\n",
    "#display(correlations_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa376817-c5a3-4287-8988-b4d5ae39cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3: object_group_text\n",
    "object_group_text = widgets.Text(\n",
    "    placeholder='/filepath/to/settings.json',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='auto', height='auto'),\n",
    ")\n",
    "#display(object_group_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90205ef4-da04-4021-a0a8-d5ea200c90f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4: remark_text\n",
    "remark_text = widgets.Text(\n",
    "    placeholder=f'TKT000123 <username> {datetime.datetime.now().date()}',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='auto', height='auto'),\n",
    ")\n",
    "#display(remark_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5371b3-a5e5-4b1e-a616-71631d9079cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5: generate_button\n",
    "generate_button = widgets.Button(\n",
    "    description='Generate',\n",
    "    tooltip='Generate Output',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='100%', height='80%')\n",
    ")\n",
    "\n",
    "def generate_button_onclick(_):\n",
    "    logging.info(\"-\"*15)\n",
    "    logging.info(\"INFO:__main__:generate_button_onclick:$ generate button clicked\")\n",
    "    global remark\n",
    "    global correlations_file\n",
    "    global settings_file\n",
    "    try:\n",
    "        if correlations_text.value:\n",
    "            correlations_file = correlations_text.value\n",
    "        elif not correlations_text.value:\n",
    "            correlations_file = \"correlations.json\"\n",
    "    except Exception as error:\n",
    "        logging.info(f\"INFO:__main__:generate_button_onclick:$ failed to load correlations\")\n",
    "    try:\n",
    "        if object_group_text.value:\n",
    "            settings_file = object_group_text.value\n",
    "        elif not object_group_text.value:\n",
    "            settings_file = \"settings.json\"\n",
    "    except Exception as error:\n",
    "        logging.info(f\"INFO:__main__:generate_button_onclick:$ failed to load settings\")\n",
    "    try:\n",
    "        if remark_text.value:\n",
    "            remark = remark_text.value\n",
    "        elif not remark_text.value:\n",
    "            remark = ''\n",
    "        pass_fail_indicator.value = True\n",
    "        generate_commands()\n",
    "    except Exception as error:\n",
    "        logging.info(f\"INFO:__main__:generate_button_onclick:$ command generation failed : exception {error}\")\n",
    "        pass_fail_indicator.value = False\n",
    "    try:\n",
    "        output_textarea.value = '\\n'.join(generated_commands)\n",
    "    except Exception as error:\n",
    "        logging.info(f\"INFO:__main__:generate_button_onclick:$ command output failed : exception {error}\")\n",
    "    \n",
    "generate_button.on_click(generate_button_onclick)\n",
    "    \n",
    "#display(generate_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4482f1fe-0c79-4233-82c0-fa63e1f1a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6: pass_fail_indicator\n",
    "pass_fail_indicator = widgets.Valid(\n",
    "    value=True,\n",
    "    description='Pass/Fail:',\n",
    ")\n",
    "#display(pass_fail_indicator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead3794e-cea6-475a-bab3-e619d60ac772",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 output_dataframe\n",
    "def output_dataframe(rule_dataframe):\n",
    "    out = widgets.Output(layout=widgets.Layout(width='auto', height='auto', border='3px solid gray'))\n",
    "    with out:\n",
    "        display(rule_dataframe)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7c3267-7e68-4280-a0be-5ba0e64b0642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8: output_text\n",
    "output_textarea = widgets.Textarea(placeholder='Output Commands', value='', layout=widgets.Layout(width='100%', height='100%', border='3px solid gray'))\n",
    "#display(output_textarea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2817b2-60ec-4673-b201-273b0f62c6cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid = widgets.GridspecLayout(9, 4, \n",
    "                              height='auto', \n",
    "                              width='auto',\n",
    "                              grid_gap=\"10px 10px\",\n",
    "                             layout=widgets.Layout(\n",
    "                                    grid_template_columns='',\n",
    "                                    grid_template_rows='',\n",
    "                                    border='3px solid gray'))\n",
    "\n",
    "grid[0:3, 0:] = raw_input_textarea\n",
    "grid[3:, 1:3] = output_textarea\n",
    "grid[3, 3] = correlations_text\n",
    "grid[4, 3] = object_group_text\n",
    "grid[5, 3] = remark_text\n",
    "grid[6, 3] = pass_fail_indicator\n",
    "grid[7, 3] = generate_button\n",
    "grid[3:, :1] = output_dataframe(rule_dataframe)\n",
    "display(grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
